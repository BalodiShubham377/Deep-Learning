{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4s1BfVeTCkZ"
   },
   "source": [
    "# gau-gan\n",
    "\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3960,
     "status": "ok",
     "timestamp": 1648110334208,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "FMUbhYAXTCkc"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1648110338774,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "4JL0y_lRTCkd"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "elapsed": 6614,
     "status": "ok",
     "timestamp": 1648110347474,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "k89RUkPeTCke",
    "outputId": "d3ff1325-4d4c-459b-a1a0-d05a4437942d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Please enter your API key ( from https://jovian.ai/ ):\u001b[0m\n",
      "API KEY: ··········\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/shubhambalodi79/gau-gan\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shubhambalodi79/gau-gan'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"gau-gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6566,
     "status": "ok",
     "timestamp": 1648110353988,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "z6-31TPlTCkh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class SPADE(nn.Module):\n",
    "    '''\n",
    "    SPADE Class\n",
    "    Values:\n",
    "        channels: the number of channels in the input, a scalar\n",
    "        cond_channels: the number of channels in conditional input (one-hot semantic labels), a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, cond_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(channels)\n",
    "        self.spade = nn.Sequential(\n",
    "            nn.Conv2d(cond_channels, channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, 2 * channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        # Apply normalization\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        # Compute denormalization\n",
    "        seg = F.interpolate(seg, size=x.shape[-2:], mode='nearest')\n",
    "        gamma, beta = torch.chunk(self.spade(seg), 2, dim=1)\n",
    "\n",
    "        # Apply denormalization\n",
    "        x = x * (1 + gamma) + beta\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaxmBCRUXepV"
   },
   "source": [
    "### Residual Blocks\n",
    "\n",
    "Let's now implement residual blocks with SPADE normalization. You should be familiar with the residual block by now, but this implementation will be a bit different to accomodate for the extra semantic label map input. For a refresher on residual blocks, please take a look [here](https://paperswithcode.com/method/residual-block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1648110353992,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "u0_fZw_kXLZp"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualBlock Class\n",
    "    Values:\n",
    "        in_channels: the number of input channels, a scalar\n",
    "        out_channels: the number of output channels, a scalar\n",
    "        cond_channels: the number of channels in conditional input in spade layer, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, cond_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        hid_channels = min(in_channels, out_channels)\n",
    "\n",
    "        self.proj = in_channels != out_channels\n",
    "        if self.proj:\n",
    "            self.norm0 = SPADE(in_channels, cond_channels)\n",
    "            self.conv0 = nn.utils.spectral_norm(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            )\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.norm1 = SPADE(in_channels, cond_channels)\n",
    "        self.norm2 = SPADE(hid_channels, cond_channels)\n",
    "        self.conv1 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(in_channels, hid_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.conv2 = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(hid_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        dx = self.norm1(x, seg)\n",
    "        dx = self.activation(dx)\n",
    "        dx = self.conv1(dx)\n",
    "        dx = self.norm2(dx, seg)\n",
    "        dx = self.activation(dx)\n",
    "        dx = self.conv2(dx)\n",
    "\n",
    "        # Learn skip connection if in_channels != out_channels\n",
    "        if self.proj:\n",
    "            x = self.norm0(x, seg)\n",
    "            x = self.conv0(x)\n",
    "\n",
    "        return x + dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyNGULD3aZMl"
   },
   "source": [
    "## GauGAN Parts\n",
    "\n",
    "Now that you understand the main contributions of GauGAN and its submodules, let's dive into the encoder, generator, and discriminator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmmuqGL5aazE"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "GauGAN's encoder serves a different purpose than Pix2PixHD's. Instead of learning feature maps to be fed as input to the generator, GauGAN's encoder encodes the original image into a mean and standard deviation from which to sample noise, which is given to the generator. You may recall this same technique of encoding to a mean and standard devation is used in variational autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1648110355045,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "-0CgXhHEaOR5"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder Class\n",
    "    Values:\n",
    "        spatial_size: tuple specifying (height, width) of full size image, a tuple\n",
    "        z_dim: number of dimensions of latent noise vector (z), a scalar\n",
    "        n_downsample: number of downsampling blocks in the encoder, a scalar\n",
    "        base_channels: number of channels in the last hidden layer, a scalar\n",
    "    '''\n",
    "\n",
    "    max_channels = 512\n",
    "\n",
    "    def __init__(self, spatial_size, z_dim=256, n_downsample=6, base_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        channels = base_channels\n",
    "        for i in range(n_downsample):\n",
    "            in_channels = 3 if i == 0 else channels\n",
    "            out_channels = 2 * z_dim if i < n_downsample else max(self.max_channels, channels * 2)\n",
    "            layers += [\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(in_channels, out_channels, stride=2, kernel_size=3, padding=1)\n",
    "                ),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            ]\n",
    "            channels = out_channels\n",
    "\n",
    "        h, w = spatial_size[0] // 2 ** n_downsample, spatial_size[1] // 2 ** n_downsample\n",
    "        layers += [\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(channels * h * w, 2 * z_dim),\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.chunk(self.layers(x), 2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp1VuAGseBP5"
   },
   "source": [
    "### Generator\n",
    "\n",
    "The GauGAN generator is actually very different from previous image-to-image translation generators. Because information from the semantic label map is injected at each batch normalization layer, the generator is able to just take random noise $z$ as input. This noise is reshaped and upsampled to the target image size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1648110357601,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "OeuvadRBd7wR"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        n_classes: the number of object classes in the dataset, a scalar\n",
    "        bottom_width: the downsampled spatial size of the image, a scalar\n",
    "        z_dim: the number of dimensions the z noise vector has, a scalar\n",
    "        base_channels: the number of channels in last hidden layer, a scalar\n",
    "        n_upsample: the number of upsampling operations to apply, a scalar\n",
    "    '''\n",
    "\n",
    "    max_channels = 1024\n",
    "\n",
    "    def __init__(self, n_classes, spatial_size, z_dim=256, base_channels=64, n_upsample=6):\n",
    "        super().__init__()\n",
    "\n",
    "        h, w = spatial_size[0] // 2 ** n_upsample, spatial_size[1] // 2 ** n_upsample\n",
    "        self.proj_z = nn.Linear(z_dim, self.max_channels * h * w)\n",
    "        self.reshape = lambda x: torch.reshape(x, (-1, self.max_channels, h, w))\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(n_upsample)):\n",
    "            in_channels = min(self.max_channels, base_channels * 2 ** (i+1))\n",
    "            out_channels = min(self.max_channels, base_channels * 2 ** i)\n",
    "            self.res_blocks.append(ResidualBlock(in_channels, out_channels, n_classes))\n",
    "\n",
    "        self.proj_o = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, seg):\n",
    "        h = self.proj_z(z)\n",
    "        h = self.reshape(h)\n",
    "        for res_block in self.res_blocks:\n",
    "            h = res_block(h, seg)\n",
    "            h = self.upsample(h)\n",
    "        h = self.proj_o(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za7PpvzQj200"
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "The architecture of the discriminator follows the one used in Pix2PixHD, which uses a multi-scale design with the InstanceNorm. The only difference here is that they apply spectral normalization to all convolutional layers. GauGAN's discriminator also takes as input the image concatenated with the semantic label map (no instance boundary map as in Pix2PixHD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648110360046,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "ArvsUmSFjp3G"
   },
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    '''\n",
    "    PatchGANDiscriminator Class\n",
    "    Implements the discriminator class for a subdiscriminator, \n",
    "    which can be used for all the different scales, just with different argument values.\n",
    "    Values:\n",
    "        in_channels: the number of channels in input, a scalar\n",
    "        base_channels: the number of channels in first convolutional layer, a scalar\n",
    "        n_layers: the number of convolutional layers, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, base_channels=64, n_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use nn.ModuleList so we can output intermediate values for loss.\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(in_channels, base_channels, kernel_size=4, stride=2, padding=2)\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Downsampling convolutional layers\n",
    "        channels = base_channels\n",
    "        for _ in range(1, n_layers):\n",
    "            prev_channels = channels\n",
    "            channels = min(2 * channels, 512)\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.utils.spectral_norm(\n",
    "                        nn.Conv2d(prev_channels, channels, kernel_size=4, stride=2, padding=2)\n",
    "                    ),\n",
    "                    nn.InstanceNorm2d(channels, affine=False),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output convolutional layer\n",
    "        prev_channels = channels\n",
    "        channels = min(2 * channels, 512)\n",
    "        self.layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(prev_channels, channels, kernel_size=4, stride=1, padding=2))\n",
    "                ,\n",
    "                nn.InstanceNorm2d(channels, affine=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 1, kernel_size=4, stride=1, padding=2)\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [] # for feature matching loss\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            outputs.append(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc-8VfSjnrri"
   },
   "source": [
    "Now you're ready to implement the multiscale discriminator in full! This puts together the different subdiscriminator scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 2955,
     "status": "ok",
     "timestamp": 1648110367255,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "XelTZvKhnVqI",
    "outputId": "090218a7-8824-41ac-ab6a-0bf8ad9f76fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/shubhambalodi79/gau-gan\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shubhambalodi79/gau-gan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1648110368001,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "6AF70gWUn2YU"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "        in_channels: number of input channels to each discriminator, a scalar\n",
    "        base_channels: number of channels in last hidden layer, a scalar\n",
    "        n_layers: number of downsampling layers in each discriminator, a scalar\n",
    "        n_discriminators: number of discriminators at different scales, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, base_channels=64, n_layers=3, n_discriminators=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize all discriminators\n",
    "        self.discriminators = nn.ModuleList()\n",
    "        for _ in range(n_discriminators):\n",
    "            self.discriminators.append(\n",
    "                PatchGANDiscriminator(in_channels, base_channels=base_channels, n_layers=n_layers)\n",
    "            )\n",
    "\n",
    "        # Downsampling layer to pass inputs between discriminators at different scales\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "\n",
    "        for i, discriminator in enumerate(self.discriminators):\n",
    "            # Downsample input for subsequent discriminators\n",
    "            if i != 0:\n",
    "                x = self.downsample(x)\n",
    "\n",
    "            outputs.append(discriminator(x))\n",
    "\n",
    "        # Return list of multiscale discriminator outputs\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def n_discriminators(self):\n",
    "        return len(self.discriminators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwpqU1vAreIR"
   },
   "source": [
    "## GauGAN: Putting it all together\n",
    "\n",
    "You can now create your GauGAN model that encapsulates all the parts you've just learned about! Since the encoder outputs mean and log-variance values to sample random noise from, this implementation will use the 'reparameterization trick' to allow gradient flow to the encoder. If you're not familiar with this trick, it samples from $\\mathcal{N}(0, I)$ and applies shift and scale ($\\mu, \\sigma$) as opposed to sampling directly from $\\mathcal{N}(\\mu, \\sigma^2I)$:\n",
    "\n",
    "\\begin{align*}\n",
    "    z: z \\sim \\mathcal{N}(\\mu, \\sigma^2I) \\equiv \\sigma * z + \\mu: z \\sim \\mathcal{N}(0, I).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1648110377009,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "DljGhpLDrYJT"
   },
   "outputs": [],
   "source": [
    "class GauGAN(nn.Module):\n",
    "    '''\n",
    "    GauGAN Class\n",
    "    Values:\n",
    "        n_classes: number of object classes in dataset, a scalar\n",
    "        spatial_size: tuple containing (height, width) of full-size image, a tuple\n",
    "        base_channels: number of channels in last generator & first discriminator layers, a scalar\n",
    "        z_dim: number of dimensions in noise vector (z), a scalar\n",
    "        n_upsample: number of downsampling (encoder) and upsampling (generator) operations, a scalar\n",
    "        n_disc_layer:: number of discriminator layers, a scalar\n",
    "        n_disc: number of discriminators (at different scales), a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        spatial_size,\n",
    "        base_channels=64,\n",
    "        z_dim=256,\n",
    "        n_upsample=6,\n",
    "        n_disc_layers=3,\n",
    "        n_disc=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            spatial_size, z_dim=z_dim, n_downsample=n_upsample, base_channels=base_channels,\n",
    "        )\n",
    "        self.generator = Generator(\n",
    "            n_classes, spatial_size, z_dim=z_dim, base_channels=base_channels, n_upsample=n_upsample,\n",
    "        )\n",
    "        self.discriminator = Discriminator(\n",
    "            n_classes + 3, base_channels=base_channels, n_layers=n_disc_layers, n_discriminators=n_disc,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        ''' Performs a full forward pass for training. '''\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "        x_fake = self.generate(z, seg)\n",
    "        pred = self.discriminate(x_fake, seg)\n",
    "        return x_fake, pred\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "        \n",
    "\n",
    "    def generate(self, z, seg):\n",
    "        ''' Generates fake image from noise vector and segmentation. '''\n",
    "        return self.generator(z, seg)\n",
    "\n",
    "    def discriminate(self, x, seg):\n",
    "        ''' Predicts whether input image is real. '''\n",
    "        return self.discriminator(torch.cat((x, seg), dim=1))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_z(mu, logvar):\n",
    "        ''' Samples noise vector with reparameterization trick. '''\n",
    "        eps = torch.randn(mu.size(), device=mu.device).to(mu.dtype)\n",
    "        return (logvar / 2).exp() * eps + mu\n",
    "\n",
    "    @property\n",
    "    def n_disc(self):\n",
    "        return self.discriminator.n_discriminators\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3cAKdo4umzH"
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "GauGAN reuses the composite loss functions that Pix2PixHD does, except it replaces the LSGAN loss with [Hinge loss](https://paperswithcode.com/method/gan-hinge-loss). It also imposes a soft (0.05 weight) Kullbach-Leibler divergence (KLD) loss term on the Gaussian statistics generated by the encoder.\n",
    "\n",
    "#### A debrief on KLD\n",
    "\n",
    "KLD measures how different two probability distributions are. In the case of $\\mathcal{N}(\\mu, \\sigma^2I)$ learned by the encoder, KLD loss encourages the learned distribution to be close to a standard Gaussian. For more information on implementation, check out Pytorch's [KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 3927,
     "status": "ok",
     "timestamp": 1648105275963,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "-ARQdYdYuXmb",
    "outputId": "9bd0d8ed-e1a8-4d87-eedc-399c32ab7465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/shubhambalodi79/gau-gan\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/shubhambalodi79/gau-gan'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1648110379811,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "aAvPulOJuq7H"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class VGG19(nn.Module):\n",
    "    '''\n",
    "    VGG19 Class\n",
    "    Wrapper for pretrained torchvision.models.vgg19 to output intermediate feature maps\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg_features = models.vgg19(pretrained=True).features\n",
    "\n",
    "        self.f1 = nn.Sequential(*[vgg_features[x] for x in range(2)])\n",
    "        self.f2 = nn.Sequential(*[vgg_features[x] for x in range(2, 7)])\n",
    "        self.f3 = nn.Sequential(*[vgg_features[x] for x in range(7, 12)])\n",
    "        self.f4 = nn.Sequential(*[vgg_features[x] for x in range(12, 21)])\n",
    "        self.f5 = nn.Sequential(*[vgg_features[x] for x in range(21, 30)])\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.f1(x)\n",
    "        h2 = self.f2(h1)\n",
    "        h3 = self.f3(h2)\n",
    "        h4 = self.f4(h3)\n",
    "        h5 = self.f5(h4)\n",
    "        return [h1, h2, h3, h4, h5]\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    '''\n",
    "    Loss Class\n",
    "    Implements composite loss for GauGAN\n",
    "    Values:\n",
    "        lambda1: weight for feature matching loss, a float\n",
    "        lambda2: weight for vgg perceptual loss, a float\n",
    "        lambda3: weight for KLD loss, a float\n",
    "        device: 'cuda' or 'cpu' for hardware to use\n",
    "        norm_weight_to_one: whether to normalize weights to (0, 1], a bool\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lambda1=10., lambda2=10., lambda3=0.05, device='cuda', norm_weight_to_one=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vgg = VGG19().to(device)\n",
    "        self.vgg_weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "        lambda0 = 1.0\n",
    "        # Keep ratio of composite loss, but scale down max to 1.0\n",
    "        scale = max(lambda0, lambda1, lambda2, lambda3) if norm_weight_to_one else 1.0\n",
    "\n",
    "        self.lambda0 = lambda0 / scale\n",
    "        self.lambda1 = lambda1 / scale\n",
    "        self.lambda2 = lambda2 / scale\n",
    "        self.lambda3 = lambda3 / scale\n",
    "\n",
    "    def kld_loss(self, mu, logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())\n",
    "\n",
    "    def g_adv_loss(self, discriminator_preds):\n",
    "        adv_loss = 0.0\n",
    "        for preds in discriminator_preds:\n",
    "            pred = preds[-1]\n",
    "            adv_loss += -pred.mean()\n",
    "        return adv_loss\n",
    "\n",
    "    def d_adv_loss(self, discriminator_preds, is_real):\n",
    "        adv_loss = 0.0\n",
    "        for preds in discriminator_preds:\n",
    "            pred = preds[-1]\n",
    "            target = -1 + pred if is_real else -1 - pred\n",
    "            mask = target < 0\n",
    "            adv_loss += (mask * target).mean()\n",
    "        return adv_loss\n",
    "\n",
    "    def fm_loss(self, real_preds, fake_preds):\n",
    "        fm_loss = 0.0\n",
    "        for real_features, fake_features in zip(real_preds, fake_preds):\n",
    "            for real_feature, fake_feature in zip(real_features, fake_features):\n",
    "                fm_loss += F.l1_loss(real_feature.detach(), fake_feature)\n",
    "        return fm_loss\n",
    "\n",
    "    def vgg_loss(self, x_real, x_fake):\n",
    "        vgg_real = self.vgg(x_real)\n",
    "        vgg_fake = self.vgg(x_fake)\n",
    "\n",
    "        vgg_loss = 0.0\n",
    "        for real, fake, weight in zip(vgg_real, vgg_fake, self.vgg_weights):\n",
    "            vgg_loss += weight * F.l1_loss(real.detach(), fake)\n",
    "        return vgg_loss\n",
    "\n",
    "    def forward(self, x_real, label_map, gaugan):\n",
    "        '''\n",
    "        Function that computes the forward pass and total loss for GauGAN.\n",
    "        '''\n",
    "        mu, logvar = gaugan.encode(x_real)\n",
    "        z = gaugan.sample_z(mu, logvar)\n",
    "        x_fake = gaugan.generate(z, label_map)\n",
    "\n",
    "        # Get necessary outputs for loss/backprop for both generator and discriminator\n",
    "        fake_preds_for_g = gaugan.discriminate(x_fake, label_map)\n",
    "        fake_preds_for_d = gaugan.discriminate(x_fake.detach(), label_map)\n",
    "        real_preds_for_d = gaugan.discriminate(x_real.detach(), label_map)\n",
    "\n",
    "        g_loss = (\n",
    "            self.lambda0 * self.g_adv_loss(fake_preds_for_g) + \\\n",
    "            self.lambda1 * self.fm_loss(real_preds_for_d, fake_preds_for_g) / gaugan.n_disc + \\\n",
    "            self.lambda2 * self.vgg_loss(x_fake, x_real) + \\\n",
    "            self.lambda3 * self.kld_loss(mu, logvar)\n",
    "        )\n",
    "        d_loss = 0.5 * (\n",
    "            self.d_adv_loss(real_preds_for_d, True) + \\\n",
    "            self.d_adv_loss(fake_preds_for_d, False)\n",
    "        )\n",
    "\n",
    "        return g_loss, d_loss, x_fake.detach()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16948,
     "status": "ok",
     "timestamp": 1648108934890,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "y3PQh37dyUEr",
    "outputId": "0796d547-b652-4b70-9545-40df64bbceb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your Cityscapes username? (http://cityscapes-dataset.com) balodi\n",
      "What is your Cityscapes password? ··········\n",
      "--2022-03-24 08:02:12--  https://www.cityscapes-dataset.com/file-handling/?packageID=1\n",
      "Resolving www.cityscapes-dataset.com (www.cityscapes-dataset.com)... 139.19.217.8\n",
      "Connecting to www.cityscapes-dataset.com (www.cityscapes-dataset.com)|139.19.217.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.cityscapes-dataset.com/login/ [following]\n",
      "--2022-03-24 08:02:13--  https://www.cityscapes-dataset.com/login/\n",
      "Reusing existing connection to www.cityscapes-dataset.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘index.html’\n",
      "\n",
      "index.html              [ <=>                ]  26.12K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-03-24 08:02:13 (279 KB/s) - ‘index.html’ saved [26748]\n",
      "\n",
      "--2022-03-24 08:02:13--  https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
      "Resolving www.cityscapes-dataset.com (www.cityscapes-dataset.com)... 139.19.217.8\n",
      "Connecting to www.cityscapes-dataset.com (www.cityscapes-dataset.com)|139.19.217.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.cityscapes-dataset.com/login/ [following]\n",
      "--2022-03-24 08:02:14--  https://www.cityscapes-dataset.com/login/\n",
      "Reusing existing connection to www.cityscapes-dataset.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘index.html.1’\n",
      "\n",
      "index.html.1            [ <=>                ]  26.12K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-03-24 08:02:14 (279 KB/s) - ‘index.html.1’ saved [26748]\n",
      "\n",
      "unzip:  cannot find or open leftImg8bit_trainvaltest, leftImg8bit_trainvaltest.zip or leftImg8bit_trainvaltest.ZIP.\n",
      "unzip:  cannot find or open gtFine_trainvaltest, gtFine_trainvaltest.zip or gtFine_trainvaltest.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Store necessary cookies\n",
    "import getpass, os\n",
    "username = input(\"What is your Cityscapes username? (http://cityscapes-dataset.com) \")\n",
    "password = getpass.getpass(\"What is your Cityscapes password? \")\n",
    "os.mkdir(\"data\")\n",
    "os.system(f\"wget --keep-session-cookies --save-cookies=data/cookies.txt --post-data 'username={username}&password={password}&submit=Login' https://www.cityscapes-dataset.com/login/\")\n",
    "# Download data\n",
    "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1\n",
    "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
    "# Unzip data\n",
    "!cd data; unzip leftImg8bit_trainvaltest\n",
    "!cd data; unzip gtFine_trainvaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 6885,
     "status": "error",
     "timestamp": 1648110099462,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "gPmVfnsBAYCZ",
    "outputId": "c085738f-4602-45c2-adab-22be93f17a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyunpack in /usr/local/lib/python3.7/dist-packages (0.2.2)\n",
      "Requirement already satisfied: entrypoint2 in /usr/local/lib/python3.7/dist-packages (from pyunpack) (1.0)\n",
      "Requirement already satisfied: easyprocess in /usr/local/lib/python3.7/dist-packages (from pyunpack) (1.1)\n",
      "Requirement already satisfied: patool in /usr/local/lib/python3.7/dist-packages (1.12)\n"
     ]
    },
    {
     "ename": "PatoolError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPatoolError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9d95f36db6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install patool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyunpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArchive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mArchive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facades_data.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyunpack/__init__.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, directory, auto_create_dir, patool_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall_patool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatool_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"zipfile\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyunpack/__init__.py\u001b[0m in \u001b[0;36mextractall_patool\u001b[0;34m(self, directory, patool_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"patool timeout\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"patool can not unpack\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextractall_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPatoolError\u001b[0m: patool can not unpack\n\nERRORS:\nUnexpected end of archive\n\nERROR: Data Error : facades_data/cmp_b0338.jpg\npatool error: error extracting /content/facades_data.zip: Command `['/usr/bin/7z', 'x', '-y', '-o/content/dataset1', '--', '/content/facades_data.zip']' returned non-zero exit status 2"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1648108990891,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "9o8Fyr6E2ozg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CityscapesDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    CityscapesDataset Class\n",
    "    Values:\n",
    "        paths: (a list of) paths to construct dataset from, a list or string\n",
    "        img_size: tuple containing the (height, width) for resizing, a tuple\n",
    "        n_classes: the number of object classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, paths, img_size=(256, 512), n_classes=35):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Collect list of examples\n",
    "        self.examples = {}\n",
    "        if type(paths) == str:\n",
    "            self.load_examples_from_dir(paths)\n",
    "        elif type(paths) == list:\n",
    "            for path in paths:\n",
    "                self.load_examples_from_dir(path)\n",
    "        else:\n",
    "            raise ValueError('`paths` should be a single path or list of paths')\n",
    "\n",
    "        self.examples = list(self.examples.values())\n",
    "        assert all(len(example) == 2 for example in self.examples)\n",
    "\n",
    "        # Initialize transforms for the real color image\n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.Lambda(lambda img: np.array(img)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "        # Initialize transforms for semantic label maps\n",
    "        self.map_transforms = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.Lambda(lambda img: np.array(img)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def load_examples_from_dir(self, abs_path):\n",
    "        '''\n",
    "        Given a folder of examples, this function returns a list of paired examples.\n",
    "        '''\n",
    "        assert os.path.isdir(abs_path)\n",
    "\n",
    "        img_suffix = '_leftImg8bit.png'\n",
    "        label_suffix = '_gtFine_labelIds.png'\n",
    "\n",
    "        for root, _, files in os.walk(abs_path):\n",
    "            for f in files:\n",
    "                if f.endswith(img_suffix):\n",
    "                    prefix = f[:-len(img_suffix)]\n",
    "                    attr = 'orig_img'\n",
    "                elif f.endswith(label_suffix):\n",
    "                    prefix = f[:-len(label_suffix)]\n",
    "                    attr = 'label_map'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if prefix not in self.examples.keys():\n",
    "                    self.examples[prefix] = {}\n",
    "                self.examples[prefix][attr] = root + '/' + f\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "\n",
    "        # Load image and maps\n",
    "        img = Image.open(example['orig_img']).convert('RGB') # color image: (3, h, w)\n",
    "        label = Image.open(example['label_map'])             # semantic label map: (1, h, w)\n",
    "\n",
    "        # Apply corresponding transforms\n",
    "        img = self.img_transforms(img)\n",
    "        label = self.map_transforms(label).long() * 255\n",
    "\n",
    "        # Convert labels to one-hot vectors\n",
    "        label = F.one_hot(label, num_classes=self.n_classes)\n",
    "        label = label.squeeze(0).permute(2, 0, 1).to(img.dtype)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        imgs, labels = [], []\n",
    "        for (x, l) in batch:\n",
    "            imgs.append(x)\n",
    "            labels.append(l)\n",
    "        return torch.stack(imgs, dim=0), torch.stack(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "executionInfo": {
     "elapsed": 3556,
     "status": "error",
     "timestamp": 1648109050647,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "S0JE0D_j3mEw",
    "outputId": "cebad8f4-85ca-4126-9bed-ce8e53c7c081"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b298e8df5a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCityscapesDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 103\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    ''' Function for scheduling learning rate '''\n",
    "    return 1. if epoch < decay_after else 1 - float(epoch - decay_after) / (epochs - decay_after)\n",
    "\n",
    "def weights_init(m):\n",
    "    ''' Function for initializing all model weights '''\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weights_init)\n",
    "\n",
    "# Initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gaugan_config = {\n",
    "    'n_classes': 35,\n",
    "    'spatial_size': (128, 256), # Default (256, 512): halve size for memory\n",
    "    'base_channels': 32,        # Default 64: halve channels for memory\n",
    "    'z_dim': 256,\n",
    "    'n_upsample': 5,            # Default 6: decrease layers for memory\n",
    "    'n_disc_layers': 2,\n",
    "    'n_disc': 3,\n",
    "}\n",
    "gaugan = GauGAN(**gaugan_config).to(device)\n",
    "loss = Loss(device=device)\n",
    "\n",
    "# Initialize dataloader\n",
    "train_dir = ['data']\n",
    "batch_size = 16                 # Default 32: decrease for memory\n",
    "dataset = CityscapesDataset(\n",
    "    train_dir, img_size=gaugan_config['spatial_size'], n_classes=gaugan_config['n_classes'],\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset, collate_fn=CityscapesDataset.collate_fn,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    drop_last=False, pin_memory=True,\n",
    ")\n",
    "\n",
    "# Initialize optimizers + schedulers\n",
    "epochs = 200                    # total number of train epochs\n",
    "decay_after = 100               # number of epochs with constant lr\n",
    "betas = (0.0, 0.999)\n",
    "\n",
    "g_params = list(gaugan.generator.parameters()) + list(gaugan.encoder.parameters())\n",
    "d_params = list(gaugan.discriminator.parameters())\n",
    "\n",
    "g_optimizer = torch.optim.Adam(g_params, lr=1e-4, betas=betas)\n",
    "d_optimizer = torch.optim.Adam(d_params, lr=4e-4, betas=betas)\n",
    "g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lr_lambda)\n",
    "d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "error",
     "timestamp": 1648109014809,
     "user": {
      "displayName": "Shubham Balodi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12667125085737334074"
     },
     "user_tz": -330
    },
    "id": "jh1Xg-123qws",
    "outputId": "f0d0c759-dedc-43a9-9feb-6655a347572b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-86a9cb8bf79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m train(\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaugan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mg_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parse torch version for autocast\n",
    "# #################################################\n",
    "version = torch.__version__\n",
    "version = tuple(int(n) for n in version.split('.')[:-1])\n",
    "has_autocast = version >= (1, 6)\n",
    "# #################################################\n",
    "\n",
    "def show_tensor_images(image_tensor):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:1], nrow=1)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def train(dataloader, gaugan, optimizers, schedulers, device):\n",
    "    g_optimizer, d_optimizer = optimizers\n",
    "    g_scheduler, d_scheduler = schedulers\n",
    "\n",
    "    cur_step = 0\n",
    "    display_step = 100\n",
    "\n",
    "    mean_g_loss = 0.0\n",
    "    mean_d_loss = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training epoch\n",
    "        for (x_real, labels) in tqdm(dataloader, position=0):\n",
    "            x_real = x_real.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
    "            # If you're running older versions of torch, comment this out\n",
    "            # and use NVIDIA apex for mixed/half precision training\n",
    "            if has_autocast:\n",
    "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                    g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
    "            else:\n",
    "                g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            mean_g_loss += g_loss.item() / display_step\n",
    "            mean_d_loss += d_loss.item() / display_step\n",
    "\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'\n",
    "                      .format(cur_step, mean_g_loss, mean_d_loss))\n",
    "                show_tensor_images(x_fake.to(x_real.dtype))\n",
    "                show_tensor_images(x_real)\n",
    "                mean_g_loss = 0.0\n",
    "                mean_d_loss = 0.0\n",
    "            cur_step += 1\n",
    "\n",
    "        g_scheduler.step()\n",
    "        d_scheduler.step()\n",
    "\n",
    "train(\n",
    "    dataloader, gaugan,\n",
    "    [g_optimizer, d_optimizer],\n",
    "    [g_scheduler, d_scheduler],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2sa5CU76xCP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "gau-gan.ipynb",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}