{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4GjM6EtNw3g"
   },
   "source": [
    "# Stylegan2\n",
    "\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbBQIV71Nw3l"
   },
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6TNThndNw3n"
   },
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIpW9ZQfNw3o"
   },
   "outputs": [],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"stylegan2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMyqK5zJOxoW"
   },
   "source": [
    "In this notebook, you're going to learn about StyleGAN2, from the paper [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958) (Karras et al., 2019), and how it builds on StyleGAN. This is the V2 of StyleGAN, so be prepared for even more extraordinary outputs. Here's the quick version: \n",
    "\n",
    "1. **Demodulation.** The instance normalization of AdaIN in the original StyleGAN actually was producing “droplet artifacts” that made the output images clearly fake. AdaIN is modified a bit in StyleGAN2 to make this not happen. Below, *Figure 1* from the StyleGAN2 paper is reproduced, showing the droplet artifacts in StyleGAN. \n",
    "\n",
    "![droplet artifacts example](droplet_artifact.png)\n",
    "\n",
    "2. **Path length regularization.** (Perceptual path length) was introduced in the original StyleGAN paper, as a metric for measuring the disentanglement of the intermediate noise space W. PPL measures the change in the output image, when interpolating between intermediate noise vectors $w$. You'd expect a good model to have a smooth transition during interpolation, where the same step size in $w$ maps onto the same amount of perceived change in the resulting image. \n",
    "\n",
    "  Using this intuition, you can make the mapping from $W$ space to images smoother, by  encouraging a given change in $w$ to correspond to a constant amount of change in the image. This is known as path length regularization, and as you might expect, included as a term in the loss function. This smoothness also made the generator model \"significantly easier to invert\"! Recall that inversion means going from a real or fake image to finding its $w$, so you can easily adapt the image's styles by controlling $w$.\n",
    "\n",
    "\n",
    "3. **No progressive growing.** While progressive growing was seemingly helpful for training the network more efficiently and with greater stability at lower resolutions before progressing to higher resolutions, there's actually a better way. Instead, you can replace it with 1) a better neural network architecture with skip and residual connections (which you also see in Course 3 models, Pix2Pix and CycleGAN), and 2) training with all of the resolutions  at once, but gradually moving the generator's _attention_ from lower-resolution to higher-resolution dimensions. So in a way, still being very careful about how to handle different resolutions to make training eaiser, from lower to higher scales.\n",
    "\n",
    "There are also a number of performance optimizations, like calculating the regularization less frequently. We won't focus on those in this notebook, but they are meaningful technical contributions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf0y63axNw3q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images,\n",
    "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=2)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW996zrBPQ0R"
   },
   "source": [
    "## Fixing Instance Norm\n",
    "One issue with instance normalization is that it can lose important information that is typically communicated by relative magnitudes. In StyleGAN2, it was proposed that the droplet artifects are a way for the network to \"sneak\" this magnitude information with a single large spike. This issue was also highlighted in the paper which introduced GauGAN, [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291) (Park et al.), earlier in 2019. In that more extreme case, instance normalization could sometimes eliminate all semantic information, as shown in their paper's *Figure 3*: \n",
    "\n",
    "![information loss by gaugan](gaugan_in.png)\n",
    "\n",
    "\n",
    "While removing normalization is technically possible, it reduces the controllability of the model, a major feature of StyleGAN. Here's one solution from the paper:\n",
    "\n",
    "### Output Demodulation\n",
    "The first solution notes that the scaling the output of a convolutional layer by style has a consistent and numerically reproducible impact on the standard deviation of its output. By scaling down the standard deviation of the output to 1, the droplet effect can be reduced. \n",
    "\n",
    "More specifically, the style $s$, when applied as a multiple to convolutional weights $w$, resulting in weights $w'_{ijk}=s_i \\cdot w_{ijk}$ will have standard deviation $\\sigma_j = \\sqrt{\\sum_{i,k} w'^2_{ijk}}$. One can simply divide the output of the convolution by this factor. \n",
    "\n",
    "However, the authors note that dividing by this factor can also be incorporated directly into the the convolutional weights (with an added $\\epsilon$ for numerical stability):\n",
    "\n",
    "$$w''_{ijk}=\\frac{w'_{ijk}}{\\sqrt{\\sum_{i,k} w'^2_{ijk} + \\epsilon}}$$\n",
    "\n",
    "This makes it so that this entire operation can be baked into a single convolutional layer, making it easier to work with, implement, and integrate into the existing architecture of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeHDb70rPR3w"
   },
   "outputs": [],
   "source": [
    "class ModulatedConv2d(nn.Module):\n",
    "    '''\n",
    "    ModulatedConv2d Class, extends/subclass of nn.Module\n",
    "    Values:\n",
    "      channels: the number of channels the image has, a scalar\n",
    "      w_dim: the dimension of the intermediate tensor, w, a scalar \n",
    "    '''\n",
    "\n",
    "    def __init__(self, w_dim, in_channels, out_channels, kernel_size, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv_weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.style_scale_transform = nn.Linear(w_dim, in_channels)\n",
    "        self.eps = 1e-6\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, image, w):\n",
    "        # There is a more efficient (vectorized) way to do this using the group parameter of F.conv2d,\n",
    "        # but for simplicity and readibility you will go through one image at a time.\n",
    "        images = []\n",
    "        for i, w_cur in enumerate(w):\n",
    "            # Calculate the style scale factor\n",
    "            style_scale = self.style_scale_transform(w_cur)\n",
    "            # Multiply it by the corresponding weight to get the new weights\n",
    "            w_prime = self.conv_weight * style_scale[None, :, None, None]\n",
    "            # Demodulate the new weights based on the above formula\n",
    "            w_prime_prime = w_prime / torch.sqrt(\n",
    "                (w_prime ** 2).sum([1, 2, 3])[:, None, None, None] + self.eps\n",
    "            )\n",
    "            images.append(F.conv2d(image[i][None], w_prime_prime, padding=self.padding))\n",
    "        return torch.cat(images)\n",
    "    \n",
    "    def forward_efficient(self, image, w):\n",
    "        # Here's the more efficient approach. It starts off mostly the same\n",
    "        style_scale = self.style_scale_transform(w)\n",
    "        w_prime = self.conv_weight[None] * style_scale[:, None, :, None, None]\n",
    "        w_prime_prime = w_prime / torch.sqrt(\n",
    "            (w_prime ** 2).sum([2, 3, 4])[:, :, None, None, None] + self.eps\n",
    "        )\n",
    "        # Now, the trick is that we'll make the images into one image, and \n",
    "        # all of the conv filters into one filter, and then use the \"groups\"\n",
    "        # parameter of F.conv2d to apply them all at once\n",
    "        batchsize, in_channels, height, width = image.shape\n",
    "        out_channels = w_prime_prime.shape[2]\n",
    "        # Create an \"image\" where all the channels of the images are in one sequence\n",
    "        efficient_image = image.view(1, batchsize * in_channels, height, width)\n",
    "        efficient_filter = w_prime_prime.view(batchsize * out_channels, in_channels, *w_prime_prime.shape[3:])\n",
    "        efficient_out = F.conv2d(efficient_image, efficient_filter, padding=self.padding, groups=batchsize)\n",
    "        return efficient_out.view(batchsize, out_channels, *image.shape[2:])\n",
    "\n",
    "example_modulated_conv = ModulatedConv2d(w_dim=128, in_channels=3, out_channels=3, kernel_size=3)\n",
    "num_ex = 2\n",
    "image_size = 64\n",
    "rand_image = torch.randn(num_ex, 3, image_size, image_size) # A 64x64 image with 3 channels\n",
    "rand_w = torch.randn(num_ex, 128)\n",
    "new_image = example_modulated_conv(rand_image, rand_w)\n",
    "second_modulated_conv = ModulatedConv2d(w_dim=128, in_channels=3, out_channels=3, kernel_size=3)\n",
    "second_image = second_modulated_conv(new_image, rand_w)\n",
    "\n",
    "print(\"Original noise (left), noise after modulated convolution (middle), noise after two modulated convolutions (right)\")\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "show_tensor_images(torch.stack([rand_image, new_image, second_image], 1).view(-1, 3, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAIyTA8YS_0r"
   },
   "source": [
    "## Path Length Regularization\n",
    "\n",
    "Path length regularization was introduced based on the usefulness of PPL, or perceptual path length, a metric used of evaluating disentanglement proposed in the original StyleGAN paper --  for a detailed overview! In essence, for a fixed-size step in any direction in $W$ space, the metric attempts to make the change in image space to have a constant magnitude $a$. This is accomplished (in theory) by first taking the Jacobian of the generator with respect to $w$, which is $\\mathop{\\mathrm{J}_{\\mathrm{w}}}={\\partial g(\\mathrm{w})} / {\\partial \\mathrm{w}}$. \n",
    "\n",
    "Then, you take the L2 norm of Jacobian matrix and you multiply that by random images (that you sample from a normal distribution, as you often do): \n",
    "$\\Vert \\mathrm{J}_{\\mathrm{w}}^T \\mathrm{y} \\Vert_2$. This captures the expected magnitude of the change in pixel space. From this, you get a loss term, which penalizes the distance between this magnitude and $a$. The paper notes that this has similarities to spectral normalization  because it constrains multiple norms. \n",
    "\n",
    "An additional optimization is also possible and ultimately used in the StyleGAN2 model: instead of directly computing $\\mathrm{J}_{\\mathrm{w}}^T \\mathrm{y}$, you can more efficiently calculate the gradient \n",
    "$\\nabla_{\\mathrm{w}} (g(\\mathrm{w}) \\cdot \\mathrm{y})$.\n",
    "\n",
    "Finally, a bit of talk on $a$: $a$ is not a fixed constant, but an exponentially decaying average of the magnitudes over various runs -- as with most times you see (decaying) averages being used, this is to smooth out the value of $a$ across multiple iterations, not just dependent on one. Notationally, with decay rate $\\gamma$, $a$ at the next iteration $a_{t+1} = {a_t} * (1 - \\gamma) + \\Vert \\mathrm{J}_{\\mathrm{w}}^T \\mathrm{y} \\Vert_2 * \\gamma$. \n",
    "\n",
    "However, for your one example iteration you can treat $a$ as a constant for simplicity. There is also an example of an update of $a$ after the calculation of the loss, so you can see what $a_{t+1}$ looks like with exponential decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MHIuIzZS_VF"
   },
   "outputs": [],
   "source": [
    "# For convenience, we'll define a very simple generator here:\n",
    "class SimpleGenerator(nn.Module):\n",
    "    '''\n",
    "    SimpleGenerator Class, for path length regularization demonstration purposes\n",
    "    Values:\n",
    "      channels: the number of channels the image has, a scalar\n",
    "      w_dim: the dimension of the intermediate tensor, w, a scalar \n",
    "    '''\n",
    "\n",
    "    def __init__(self, w_dim, in_channels, hid_channels, out_channels, kernel_size, padding=1, init_size=64):\n",
    "        super().__init__()\n",
    "        self.w_dim = w_dim\n",
    "        self.init_size = init_size\n",
    "        self.in_channels = in_channels\n",
    "        self.c1 = ModulatedConv2d(w_dim, in_channels, hid_channels, kernel_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.c2 = ModulatedConv2d(w_dim, hid_channels, out_channels, kernel_size)\n",
    "\n",
    "    def forward(self, w):\n",
    "        image = torch.randn(len(w), self.in_channels, self.init_size, self.init_size).to(w.device)\n",
    "        y = self.c1(image, w)\n",
    "        y = self.activation(y)\n",
    "        y = self.c2(y, w)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_IEWvjdTQVj"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "def path_length_regulization_loss(generator, w, a):\n",
    "    # Generate the images from w\n",
    "    fake_images = generator(w)\n",
    "    # Get the corresponding random images\n",
    "    random_images = torch.randn_like(fake_images)\n",
    "    # Output variation that we'd like to regularize\n",
    "    output_var = (fake_images * random_images).sum()\n",
    "    # Calculate the gradient with respect to the inputs\n",
    "    cur_grad = grad(outputs=output_var, inputs=w)[0]\n",
    "    # Calculate the distance from a\n",
    "    penalty = (((cur_grad - a) ** 2).sum()).sqrt()\n",
    "    return penalty, output_var\n",
    "simple_gen = SimpleGenerator(w_dim=128, in_channels=3, hid_channels=64, out_channels=3, kernel_size=3)\n",
    "samples = 10\n",
    "test_w = torch.randn(samples, 128).requires_grad_()\n",
    "a = 10\n",
    "penalty, variation = path_length_regulization_loss(simple_gen, test_w, a=a)\n",
    "\n",
    "decay = 0.001 # How quickly a should decay\n",
    "new_a = a * (1 - decay) + variation * decay\n",
    "print(f\"Old a: {a}; new a: {new_a.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY86oTgjTTOn"
   },
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6g8psigToAK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "stylegan2.ipynb",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}